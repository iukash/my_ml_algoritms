{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10160829",
   "metadata": {},
   "source": [
    "## Случайный лес (беггинг)\n",
    "\n",
    "Идея беггинга заключается в том чтобы построить композицию моделей с меньшим относительно одной модели разбросом. Построение композиции происходит за счет генерации бутстрепом выборки $\\widetilde X$ из всей имеющейся выборки $X$ (где $|\\widetilde X| = d$, размеру выборки) и построения модели $b(x)$ с помощью модели $\\mu$ на выборке $\\widetilde X$.\n",
    "$$ a(x) = \\frac{1}{N}\\sum_{n=1}^N b_n(x)$$ \n",
    "где $a(x)$ - композиция моделей.\n",
    "\n",
    "Для того чтобы понять принцип работы беггинга, разложим ошибку предсказания на компоненты\n",
    "$$L(\\mu) = \\mathbb E_{x, y}(y-\\mathbb E(y|x))^2 + \\mathbb E_x (\\mathbb E_x \\mu(X) - \\mathbb E(y|x))^2 + \\mathbb E_x \\mathbb E_X(\\mu(X) - \\mathbb E_x \\mu(X))^2 $$\n",
    "где \n",
    "* 1-я компонента в формуле - шумовая компонента: показывает насколько плох лучший из возможных алгоритмов\n",
    "* 2-я компонента - смещение (bias): показывает насколько в среднем мы в состоянии приблизить лучшую модель\n",
    "* 3-я компонента - разброс (variance): отклонение модели на данной выборке относительно средней модели\n",
    "\n",
    "Воздействовать на 1-ю компоненту мы не можем по определению, на 2-ю в случае беггинга не получится в силу того что смещение композиции $a(x)$ равно смещению любой из модели $b(x)$, т.е. не уменьшится от количества моделей и зависит от выбора базовой модели (необходимо выбирать базовую модель с низким смещением), а вот влияние на 3-ю разберем подробнее. Формула разброса выглядит следующим образом:\n",
    "$$variance(a(x)) = (\\frac{1}{N} \\mathbb E_x \\mathbb E_X (\\tilde \\mu (X) - \\mathbb E_X \\tilde \\mu (X))^2)(1) + (\\frac{N*(N-1)}{N^2} \\mathbb E_x \\mathbb E_X (\\tilde \\mu_1(X) - \\mathbb E_x \\tilde \\mu_1 )(\\tilde \\mu_2(X) - \\mathbb E_x \\tilde \\mu_2 ))(2)$$\n",
    "где \n",
    "* $\\tilde \\mu (X) - \\mathbb E_X \\tilde \\mu (X)$ - разброс одной модели\n",
    "* $\\mathbb E_x$ - перебор по всем объектам множества X\n",
    "* $\\mathbb E_X$ - перебор по всем подмножествам\n",
    "* $(\\tilde \\mu_1(X) - \\mathbb E_x \\tilde \\mu_1 )$ - ковариация моделей построенных бутсрепом\n",
    "\n",
    "Из формулы разброса делаем вывод что для минизации разброса необходимо:\n",
    "1) Максимально увеличить количество моделей (для уменьшения первой компоненты формулы)\n",
    "2) Ответы моделей должны быть как можно менее скоррелированы между собой (для уменьшения второй компоненты)\n",
    "\n",
    "Исходя из поставленных требований в целях снижения смещения выбираем глубокие деревья, а в целях снижения ковариации (помимо ее снижения от бутстрепа) изменяем алгоритм работы дерева (каждый раз при выборе лучшего предиката ограничить набор признаков classification $m = \\sqrt d$ regression $m = \\frac{d}{3}$). Полученный алгоритм и является случайным лесом (Random Forest).\n",
    "\n",
    "Одним из интересных свойств случайного леса является Out-of-bag оценка. Идея заклчается в том чтобы проводить оценку по объектам, которые не попали в обучающую выборку из-за бутсрепа.\n",
    "\n",
    "Слабые стороны данного алгоритма скорость работы (длительный процесс построения большого количества глубоких деревьев)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47160496",
   "metadata": {},
   "source": [
    "### Реализация алгоритма на python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d1cf3",
   "metadata": {},
   "source": [
    "**DecisionTreesMyForRF** - класс с дополнетельным параметром m в fit для рандомизации количества признаков при поиске лучшего предиката"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b6ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт необходимых библиотек\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from DecisionTreesMyForRF import DecisionTreeRegressorMy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd0168a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#модель построения случайного леса\n",
    "class RandomForestMy():\n",
    "    #инициализация\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #обучение состоит в создании n моделей дерева решений\n",
    "    def fit(self, X, y, deep=30, n=100):\n",
    "        self.trees = []\n",
    "        for _ in range(n):\n",
    "            model_my = DecisionTreeRegressorMy()\n",
    "            sample = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=True)\n",
    "            model_my.fit(X[sample], y[sample], m=6)\n",
    "            self.trees.append(model_my)\n",
    "            \n",
    "    #выдача прогноза массиву входных векторов X\n",
    "    def predict(self, X):\n",
    "        return np.mean([i.predict(X) for i in self.trees], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0544dceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree mse 0.62401112894789\n",
      "RF mse 0.6035235893862753\n"
     ]
    }
   ],
   "source": [
    "# импортируем датасет\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# загружаем и делаем выборку в 2000\n",
    "housing = fetch_california_housing()\n",
    "sample = np.random.choice(range(housing['data'].shape[0]), size=2000, replace=True)\n",
    "X = housing['data'][sample]\n",
    "y = housing['target'][sample]\n",
    "\n",
    "#сплит данных на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n",
    "\n",
    "#построение и прогноз модели решающего дерева\n",
    "model_my = DecisionTreeRegressorMy()\n",
    "model_my.fit(X_train, y_train, max_deep=5)\n",
    "print(f'tree mse {mean_squared_error(y_test, model_my.predict(X_test))}')\n",
    "\n",
    "#построение и прогноз случайного леса\n",
    "modelRF = RandomForestMy()\n",
    "modelRF.fit(X_train, y_train)\n",
    "print(f'RF mse {mean_squared_error(y_test, modelRF.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b52ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
